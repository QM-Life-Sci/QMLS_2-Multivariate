---
title: "Non-linear Classification"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
bibliography: Multivariate.bib
csl: evolution.csl
---


```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(cowplot)
library(mvtnorm)
library(rsample)
library(e1071)
library(gt)

ggplot2::theme_set(theme_cowplot())

SD2 <- read_rds("Data/SD2.rds")
SD3 <- read_rds("Data/SD3.rds")
```


## *Linear* Discriminant Analysis

- Like a MANOVA in reverse
- Linear separation of groups

Can we do better?


## Non-linear classifiers

1. Quadratic Discriminant Analysis: `MASS::qda()`
    - Linear space is "mapped" to quadratic space
    - $X_1 + X_2 + X_1^2 + X_2^2$
    - Prone to high correlations between predictors and their squares
2. Support Vector Machines
    - Non-technical introduction [@Fielding2006-xa]
    - Technical introductions [@Hastie2009-xa; @James2013-oe]


## Support Vector Machines

- Bypasses the correlation problem via a "kernel"
    - Linear, polynomial, radial, sigmoid
- [Why is a support vector machine called a machine?](https://www.quora.com/Why-is-a-support-vector-machine-called-a-machine)

![](https://www.dtreg.com/uploaded/pageimg/SvmMargin2.jpg){fig-align="center"}

<div class="ref">
Figure from [DTReg](https://www.dtreg.com/)
</div>


## SVM in R

- [`e1071` package](https://cran.r-project.org/web/packages/e1071/index.html): `svm()`
- [kernlab](https://cran.r-project.org/web/packages/kernlab/vignettes/kernlab.pdf): `ksvm()`
- [Applied Machine Learning Using mlr3 in R](https://mlr3book.mlr-org.com/)

```{r}
#| echo: true

library(e1071) 
```


## Simulated data

```{r}
ggplot(SD3, aes(X1, X2, color = Group)) +
  geom_point(size = 3) +
  coord_equal() +
  scale_color_brewer(type = "qual", palette = "Set1")
```


## Create training and testing sets

- [`rsample` package](https://rsample.tidymodels.org/index.html)
- Also:
    - [`cvTools`](https://cran.r-project.org/web/packages/cvTools/index.html)
    - `mlr3verse`
    - others

```{r}
#| echo: true

library(rsample)

set.seed(34598734)

split <- initial_split(SD3, strata = Group, prop = 0.75) 

Training_set <- training(split) 
Test_set <- testing(split)
```


## Training and testing sets

```{r}
#| echo: true

Training_set |> count(Group)
Test_set |> count(Group)
```


## Fitting SVM to the Training set 

```{r}
#| echo: true
#| output-location: slide

SVM_fit <- svm(Group ~ ., 
               data = Training_set, 
               scale = TRUE,
               type = "C-classification", 
               kernel = "radial",
               cost = 1)
SVM_fit
```


## Predicting the Test set results 

```{r}
#| echo: true

(Group_pred <- predict(SVM_fit, newdata = Test_set[, -1]))
```


## Confusion matrix

```{r}
#| echo: true

table(Test_set[, 1], Group_pred) 
```

Percent correct

```{r}
#| echo: true

mean(Test_set[, 1] == Group_pred) * 100
```


## Predicting

```{r}
#| echo: true

Grid <- crossing(X1 = seq(min(SD3$X1), max(SD3$X1), length.out = 200),
                 X2 = seq(min(SD3$X2), max(SD3$X2), length.out = 200))
Grid <- Grid |> 
  mutate(Train_pred = predict(SVM_fit, newdata = Grid))

head(Grid)
```


## Plotting the results 

```{r}
P3 <- ggplot() +
  geom_tile(data = Grid, aes(X1, X2, fill = Train_pred), alpha = 0.25) +
  geom_point(data = Training_set, aes(X1, X2, color = Group)) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  theme(legend.position = "none") +
  labs(title = "SVM Training")

# Plotting the test data set results 
P4 <- ggplot() +
  geom_tile(data = Grid, aes(X1, X2, fill = Train_pred), alpha = 0.25) +
  geom_point(data = Test_set, aes(X1, X2, color = Group)) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  theme(legend.position = "none") +
  labs(title = "SVM Testing")

plot_grid(P3, P4)
```


## Complex patterns

```{r}
set.seed(347937)

n <- 500

X <- rmvnorm(n, mean = c(0, 0)) |> as.data.frame() |> 
  rename(X1 = V1,
         X2 = V2)

CP <- data.frame(X) |> 
  mutate(Group = if_else(X1^2 + X2^2 <= 1, "A", "B") |> factor()) |> 
  relocate(Group)

ggplot(CP, aes(X1, X2, color = Group)) +
  geom_point(size = 3) +
  coord_equal() +
  scale_color_brewer(type = "qual", palette = "Set1")

split <- initial_split(CP, strata = Group, prop = 0.75) 

Training_set <- training(split) 
Test_set <- testing(split)

# Fitting SVM to the Training set 
SVM_fit <- svm(Group ~ X1 + X2, 
               data = Training_set, 
               scale = TRUE,
               type = "C-classification", 
               kernel = "radial") 

# Predicting the Test set results 
Group_pred <- predict(SVM_fit, newdata = Test_set[, -1]) 
```


## Plotting the results 

```{r}
Grid <- crossing(X1 = seq(min(CP$X1), max(CP$X1), length.out = 200),
                 X2 = seq(min(CP$X2), max(CP$X2), length.out = 200))

Grid <- Grid |> 
  mutate(Train_pred = predict(SVM_fit, newdata = Grid))

P1 <- ggplot() +
  geom_tile(data = Grid, aes(X1, X2, fill = Train_pred), alpha = 0.25) +
  geom_point(data = Training_set, aes(X1, X2, color = Group)) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  theme(legend.position = "none") +
  labs(title = "SVM Training")

# Plotting the test data set results 
P2 <- ggplot() +
  geom_tile(data = Grid, aes(X1, X2, fill = Train_pred), alpha = 0.25) +
  geom_point(data = Test_set, aes(X1, X2, color = Group)) +
  scale_color_brewer(type = "qual", palette = "Set1") +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  theme(legend.position = "none") +
  labs(title = "SVM Testing")

plot_grid(P1, P2)
```


## Application to Fly Diets

```{r}
FD <- read_csv("./Data/PreProcessed_Expr.csv",
               show_col_types = FALSE) |> 
  dplyr::select(-patRIL) |> 
  mutate(Treat = factor(Treat)) |> 
  as.data.frame()

set.seed(94875)

split <- initial_split(FD, strata = Treat, prop = 0.75) 

Training_set <- training(split) 
Test_set <- testing(split)
```

```{r}
#| echo: true
Training_set |> count(Treat)
Test_set |> count(Treat)
```


## Visualizing gene expression levels

```{r}
ggplot(FD, aes(y = Dm01792789_g1, x = Dm01798339_g1, color = Treat)) +
         geom_point()
```


## Fitting SVM to the Training set 

```{r}
#| echo: true
#| output-location: slide

SVM_fit <- svm(Treat ~ ., 
               data = Training_set, 
               scale = TRUE,
               type = "C-classification", 
               kernel = "radial",
               cost = 2)
SVM_fit
```


## Predicting the Test set results 

```{r}
#| echo: true

(Group_pred <- predict(SVM_fit, newdata = Test_set[, -1]) )
```


## Confusion matrix

```{r}
#| echo: true

table(Test_set[, 1], Group_pred) 
```

Percent correct:

```{r}
#| echo: true

mean(Test_set[, 1] == Group_pred) * 100
```


## Plotting

```{r}
plot(SVM_fit,
     data = Training_set,
     formula = Dm01792789_g1 ~ Dm01793858_g1)
```


## Plotting

```{r}
plot(SVM_fit,
     data = Training_set,
     formula = Dm01792789_g1 ~ Dm01798339_g1)
```


## Comparing kernels

```{r}
SVM_linear <- svm(Treat ~ ., 
                  data = Training_set, 
                  scale = TRUE,
                  type = "C-classification", 
                  kernel = "linear")
Linear_Pct <- mean(Test_set[, 1] == predict(SVM_linear,
                                            newdata = Test_set[, -1])) * 100

SVM_quad <- svm(Treat ~ ., 
                  data = Training_set, 
                scale = TRUE,
                type = "C-classification", 
                kernel = "polynomial",
                degree = 2)
Quad_Pct <- mean(Test_set[, 1] == predict(SVM_quad,
                                          newdata = Test_set[, -1])) * 100

SVM_poly3 <- svm(Treat ~ ., 
                 data = Training_set, 
                 scale = TRUE,
                 type = "C-classification", 
                 kernel = "polynomial",
                 degree = 3)
Poly3_Pct <- mean(Test_set[, 1] == predict(SVM_poly3,
                                           newdata = Test_set[, -1])) * 100

SVM_radial <- svm(Treat ~ ., 
                  data = Training_set, 
                  scale = TRUE,
                  type = "C-classification", 
                  kernel = "radial")
Radial_Pct <- mean(Test_set[, 1] == predict(SVM_radial,
                                            newdata = Test_set[, -1])) * 100

SVM_sigmoid <- svm(Treat ~ ., 
                   data = Training_set, 
                   scale = TRUE,
                   type = "C-classification", 
                   kernel = "sigmoid")
Sigmoid_Pct <- mean(Test_set[, 1] == predict(SVM_sigmoid,
                                             newdata = Test_set[, -1])) * 100

Accuracy <- tribble(
  ~ Kernel, ~ Pct_Corr,
  "Linear", Linear_Pct,
  "Quadratic", Quad_Pct,
  "3rd Order Polynomial", Poly3_Pct,
  "Radial", Radial_Pct,
  "Sigmoid", Sigmoid_Pct
)

Accuracy |> gt(rowname_col = "Kernel") |> 
  cols_label(Pct_Corr = md("*Percent Correct*")) |> 
  tab_options(table.font.size = 36) |> 
  fmt_number(decimals = 1)
```

## Going further

- Continuous prediction instead of classification?
    - "Regression" with SVM is possible
- What predictors are important for classification?
    - "Feature importance"
    - [`mlr3verse` packages](https://mlr3book.mlr-org.com/)
- Other "learners": [https://mlr-org.com/learners.html](https://mlr-org.com/learners.html)
    

## References

::: {#refs}
:::

