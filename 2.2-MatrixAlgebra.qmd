---
title: "Basics of Matrix Algebra"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
bibliography: Multivariate.bib
csl: evolution.csl
---

## Major uses relevant to multivariate statistics:


```{r}
#| label: setup
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(cowplot)
ggplot2::theme_set(theme_cowplot())
set.seed(74237)

```


- Linear models can often be solved via matrix algebra
- Patterns of shared variation can be summarized & composite variables can be created via matrix algebra

For more information see Tabachnick and Fidell [-@Tabachnick2019-tl] and Bakker [-@Bakker2024-mg]


## Some basic matrix attributes

- *Square matrices*: same number of rows and columns (e.g., 2x2, 9x9)
- *Symmetric matrices*: square matrices that have the same elements in the triangles above and below the diagonal
    - Correlation and covariance matrices are symmetric
    
```{r}

CC <- round(cor(tibble(x1 = rnorm(12),
                       x2 = rnorm(12),
                       x3 = rnorm(12),
                       x4 = rnorm(12),
                       x5 = rnorm(12),
                       x6 = rnorm(12))),
            digits = 3)

CC

```


## Some basic matrix attributes

In R, you can easily extract or assign parts of matrices

```{r}
#| echo: true

diag(CC)
diag(CC) <- 10
CC

```

## Some basic matrix attributes

```{r}
#| echo: true
CC 

upper.tri(CC)

CC[upper.tri(CC)]

```


## Operations with matrices

Basic operations include:

- Elementwise addition/subtraction, multiplication/division (see lecture 1.3)
- Transposition
- Matrix multiplication 
- Inversion


## Transposition

The transpose of a matrix($A$) is denoted $A'$ or $A^{T}$.

- Flip the rows to be the columns

```{r}
#| echo: true

A <- matrix(c(3,7,1,2,5,0,4,0,8), nrow = 3)
A
A_T <- t(A)
A_T

```


## Matrix Multiplication

![](./Images/matrix_mult1.png){fig-align="center" width=80%}

<div class="ref">
From Tabachnick and Fidell [-@Tabachnick2019-tl]
</div>


## Matrix Multiplication

- In R, perform matrix multiplication with `%*%`
- `*` will perform elementwise multiplication (see lecture 1.3)

```{r}
#| echo: true

A <- matrix(c(3,7,1,2,5,0,4,0,8), nrow = 3)
B <- matrix(c(6,2,3,1,8,4,0,7,5), nrow = 3)

A%*%B

#Result for `[1,1]` : row 1 of A, column 1 of B
A[1,1]*B[1,1] + A[1,2]*B[2,1] + A[1,3]*B[3,1]

#Result for `[1,2]` : row 1 of A, column 2 of B
A[1,1]*B[1,2] + A[1,2]*B[2,2] + A[1,3]*B[3,2]
```

## Matrix product is not commutative

$$AB \neq BA$$

```{r}
#| echo: true

A %*% B
B %*% A

# Result for `[1,1]` : row 1 of B, column 1 of A
B[1, 1] * A[1, 1] + B[1, 2] * A[2, 1] + B[1, 3] * A[3, 1]

# Result for `[1,2]` : row 1 of B, column 2 of A
B[1, 1] * A[1, 2] + B[1, 2] * A[2, 2] + B[1, 3] * A[3, 2]
```


## Matrix Multiplication: $A ~ B$

- Number of columns in A must match number of rows in B.
- The following cannot be multiplied:

```{r}

M1 <- matrix(1:15, nrow=5)
M2 <- matrix(20:31, nrow=4)

```

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true
M1

```

:::

::: {.column width="50%"}

```{r}
#| echo: true

M2

```

:::

::::

```{r}
#| echo: true
#| eval: false

M1 %*% M2

```

> Error in M1 %*% M2 : non-conformable arguments


## Matrix Multiplication: $A ~ B$

- Number of columns in A must match number of rows in B.
- Resulting dimensions will be: `nrows(A)` by `ncol(B)`

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: true
M1

```

:::

::: {.column width="50%"}

```{r}
#| echo: true

t(M2)

```

:::

::::

```{r}
#| echo: true

M1 %*% t(M2)

```


## Shorthand reminder

$N$s have to match:

$$[M \times N] ~ [N \times P] $$

Results in:

$$[M \times P]$$


## $AA^{T}$ is useful for data analysis

![](./Images/matrix_trans.png){fig-align="center" width=65%}

- Diagonals are sums of squares
- Upper and lower triangles are also useful

<div class="ref">
From Tabachnick and Fidell [-@Tabachnick2019-tl]
</div>


## Inversion

- Scalar numbers: the inverse of $x$ = $x^{-1}$ = $\frac{1}{x}$
    - $x ~ \frac{1}{x} = 1$
- Matrix: the inverse of $A$ is denoted $A^{-1}$
    - $AA^{-1} = I$
    - $I$ is the identity matrix    
- Only square matrices have an inverse (but not all do)

```{r}
diag(x = 3)
```


## Inversion

- Need a matrix that satisfies: $AA^{-1} = A^{-1}A = I$
- Consider our example matrix A

```{r}
A
```

`3*B[1,1] + 2*B[2,1] + 4*B[3,1] = 1`

`7*B[1,1] + 5*B[2,1] + 0*B[3,1] = 0`

`1*B[1,1] + 0*B[2,1] + 8*B[3,1] = 0`

...


## Determinant & Inversion

- Determinant of $A$ is denoted: $|A|$
- Complex calculation (several different ways)
- Useful for calculating the inverse of a matrix

![](./Images/matrix_det.png){fig-align="center" width=40%}

<div class="ref">
From Tabachnick and Fidell [-@Tabachnick2019-tl]
</div>


## Inversion with `solve()`

```{r}
#| echo: true
 
AI <- solve(A)

AI

round(A %*% AI, digits = 10)

round(AI %*% A, digits = 10)

```


## Solving for a linear model

- Most linear models are solved using basic matrix operations
- Show the calculation 
- Won't show the proof that this solution maximizes the likelihood and minimizes sums of squares 

Use it without understanding *why* it works.


## Solving for a linear model

Simulate some data:

```{r}
#| echo: true

DD <- tibble(
             x1 = rnorm(10),
             x2 = rnorm(10),
             y = rnorm(10))
mod <- lm(y ~ x1 + x2, data = DD)
summary(mod)
```


## Solving for coefficients directly

```{r}
#| echo: true

(y <- matrix(DD$y, ncol = 1))
(X <- cbind(matrix(rep(1, times = nrow(DD))),
            as.matrix(DD[, 1:2])))
```


## Solving for coefficients directly

$$B = \left(X^T ~ X \right)^{-1}~\left( X^T~y \right)$$

```{r}
#| echo: true

solve(t(X) %*% X) %*% (t(X) %*% y)
coef(mod)
```


## Solving for standardized regression coefficients

$$B_{i} = R_{ii}^{-1}R_{iy}$$
$B_{i}$ = standardized regression coefficients

$R_{ii}^{-1}$ = inverse of the correlation matrix among predictors

$R_{iy}$ = column matrix of correlations between the response and predictors


## Solving using the correlation matrix

$R_{ii}^{-1}$ = inverse of the correlation matrix among predictors

```{r}
#| echo: true

Icor <- cor(DD[ , c("x1", "x2")])
Icor_inv <- solve(Icor)

Icor
Icor_inv
```

## Solving using the correlation matrix

$R_{iy}$ = column matrix of correlations between the response and predictors

```{r}
#| echo: true

Dcor <- matrix(cor(DD)[1:2, "y"], nrow = 2)
Dcor

```


## Solving using the correlation matrix

```{r}
#| echo: true

Bst <- Icor_inv %*% Dcor
Bst
```


## Unstandardizing the coefficients

```{r}
#| echo: true
sds <- apply(DD, 2, sd)
sds <- sds['y'] / sds[c('x1', 'x2')]

Bst * sds

coef(mod)
```


## References

::: {#refs}
:::
